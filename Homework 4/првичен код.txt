import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
from datetime import datetime

BASE_URL = "https://www.mse.mk/en/stats/symbolhistory/SYMBOL?year=YEAR"
output_dir = "MSE_data"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

current_year = datetime.now().year
years_to_retrieve = 10

def filter_1_get_issuers():
    url = "https://www.mse.mk/en/stats/symbolhistory/TEL"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    issuers = [option['value'] for option in soup.select('select[name="Code"] option') if
               option['value'] and not any(char.isdigit() for char in option['value'])]
    return issuers

def filter_2_check_last_date(symbol):
    output_path = os.path.join(output_dir, f"{symbol}_data.csv")
    if os.path.exists(output_path):
        df = pd.read_csv(output_path)
        last_date = df['Date'].max()
        last_year = datetime.strptime(last_date, "%m/%d/%Y").year
        return last_year + 1
    return current_year - years_to_retrieve

def filter_3_fetch_data(symbol, year, session):
    url = BASE_URL.replace("SYMBOL", symbol).replace("YEAR", str(year))
    response = session.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        table = soup.find("table")
        if table:
            rows = table.find_all("tr")
            headers = [header.text.strip() for header in rows[0].find_all("th")]
            data = []
            for row in rows[1:]:
                columns = row.find_all("td")
                data.append([col.text.strip() for col in columns])
            df = pd.DataFrame(data, columns=headers)
            return df
    return pd.DataFrame()

def filter_4_save_data(symbol, all_data):
    output_path = os.path.join(output_dir, f"{symbol}_data.csv")
    if not all_data.empty:
        all_data.to_csv(output_path, index=False)

def process_symbol(symbol):
    start_year = current_year - years_to_retrieve
    end_year = current_year
    all_data = pd.DataFrame()
    with requests.Session() as session:
        with ThreadPoolExecutor(max_workers=12) as executor:
            futures = [executor.submit(filter_3_fetch_data, symbol, year, session) for year in
                       range(start_year, end_year + 1)]
            for future in as_completed(futures):
                yearly_data = future.result()
                all_data = pd.concat([all_data, yearly_data])
                time.sleep(0.1)
    filter_4_save_data(symbol, all_data)

def main():
    start_time = time.time()
    company_symbols = filter_1_get_issuers()
    with ThreadPoolExecutor(max_workers=8) as executor:
        executor.map(process_symbol, company_symbols)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"{elapsed_time:.2f}")

import requests.adapters

session = requests.Session()
adapter = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20)
session.mount("https://", adapter)

from concurrent.futures import ThreadPoolExecutor

def task(n):
    print(f"Processing {n}")

with ThreadPoolExecutor(max_workers=4) as executor:
    numbers = [1, 2, 3, 4, 5]
    executor.map(task, numbers)

def process_symbol(symbol, session):
    start_year = current_year - years_to_retrieve
    end_year = current_year
    all_data = pd.DataFrame()

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(filter_3_fetch_data, symbol, year, session) for year in range(start_year, end_year + 1)]
        for future in as_completed(futures):
            yearly_data = future.result()
            all_data = pd.concat([all_data, yearly_data])
            time.sleep(0.1)

    filter_4_save_data(symbol, all_data)

def main():
    start_time = time.time()
    company_symbols = filter_1_get_issuers()

    with requests.Session() as session:
        adapter = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20)
        session.mount("https://", adapter)

        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(lambda symbol: process_symbol(symbol, session), company_symbols)

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"{elapsed_time:.2f}")

from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def get_session():
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)
    session.mount("https://", adapter)
    return session

if __name__ == "__main__":
    main()
    cs=filter_1_get_issuers()
    print(cs)

pip install ta

import pandas as pd
import ta  # Technical Analysis Library
import matplotlib.pyplot as plt

news_dir = "MSE_predictions"
if not os.path.exists(news_dir):
    os.makedirs(news_dir)

def add_moving_averages(df):
    df['SMA_10'] = df['Last trade price'].rolling(window=10).mean()  # (10 дена)
    df['SMA_50'] = df['Last trade price'].rolling(window=50).mean()  # (50 дена)
    df['EMA_10'] = df['Last trade price'].ewm(span=10, adjust=False).mean()  # (10 дена)
    df['EMA_50'] = df['Last trade price'].ewm(span=50, adjust=False).mean()  # (50 дена)
    df['WMA_20'] = df['Last trade price'].rolling(window=20).apply(lambda prices: (prices * range(1, len(prices)+1)).sum() / sum(range(1, len(prices)+1)))  # (20 дена)
    return df

def add_oscillators(df):
    df['RSI'] = ta.momentum.RSIIndicator(close=df['Last trade price'], window=14).rsi()
    df['MACD'] = ta.trend.MACD(close=df['Last trade price']).macd_diff()
    df['Stochastic_K'] = ta.momentum.StochasticOscillator(high=df['Max'], low=df['Min'], close=df['Last trade price']).stoch()
    df['Stochastic_D'] = ta.momentum.StochasticOscillator(high=df['Max'], low=df['Min'], close=df['Last trade price']).stoch_signal()
    df['CCI'] = ta.trend.CCIIndicator(high=df['Max'], low=df['Min'], close=df['Last trade price'], window=20).cci()
    return df

# (buy/sell/hold)
def generate_signals(df):
    df['Signal'] = "Hold"  # Default
    df.loc[(df['RSI'] < 30) & (df['Last trade price'] > df['SMA_10']), 'Signal'] = 'Buy'
    df.loc[(df['RSI'] > 70) & (df['Last trade price'] < df['SMA_10']), 'Signal'] = 'Sell'
    df.loc[(df['EMA_10'] > df['EMA_50']) & (df['Last trade price'] > df['EMA_10']), 'Signal'] = 'Buy'
    df.loc[(df['EMA_10'] < df['EMA_50']) & (df['Last trade price'] < df['EMA_10']), 'Signal'] = 'Sell'
    return df

def analyze_timeframes(data, symbol):
    data_daily = data.copy()
    data_daily = add_moving_averages(data_daily)
    data_daily = add_oscillators(data_daily)
    data_daily = generate_signals(data_daily)

    data_weekly = data.resample('W').agg({'Max': 'max', 'Min': 'min', 'Last trade price': 'last', 'Volume': 'sum'})
    data_weekly = add_moving_averages(data_weekly)
    data_weekly = add_oscillators(data_weekly)
    data_weekly = generate_signals(data_weekly)

    data_monthly = data.resample('M').agg({'Max': 'max', 'Min': 'min', 'Last trade price': 'last', 'Volume': 'sum'})
    data_monthly = add_moving_averages(data_monthly)
    data_monthly = add_oscillators(data_monthly)
    data_monthly = generate_signals(data_monthly)

    data_daily.to_csv(f"{news_dir}/{symbol}_daily_analysis.csv", index=False)
    data_weekly.to_csv(f"{news_dir}/{symbol}_weekly_analysis.csv", index=False)
    data_monthly.to_csv(f"{news_dir}/{symbol}_monthly_analysis.csv", index=False)

    print(f"Analysis completed for {symbol} on daily, weekly, and monthly timeframes.")

    plt.figure(figsize=(14, 7))
    plt.plot(data_daily['Last trade price'], label='Close Price')
    plt.plot(data_daily['SMA_10'], label='SMA 10')
    plt.plot(data_daily['EMA_10'], label='EMA 10')
    plt.legend()
    plt.title(f"Daily Analysis for {symbol}")
    plt.show()

for symbol in cs:
    data=pd.read_csv(f"MSE_data/{symbol}_data.csv")
    data["Date"]=pd.to_datetime(data["Date"])
    data.set_index(keys=["Date"], inplace=True)

    data['Last trade price']=pd.to_numeric(data['Last trade price'].replace({',':''},regex=True))
    data['Avg. Price']=pd.to_numeric(data['Avg. Price'].replace({',':''},regex=True))
    data['Turnover in BEST in denars']=pd.to_numeric(data['Turnover in BEST in denars'].replace({',':''},regex=True))
    data['Total turnover in denars']=pd.to_numeric(data['Total turnover in denars'].replace({',':''},regex=True))
    data['Volume']=pd.to_numeric(data['Volume'].replace({',':''},regex=True))
    data['Min']=pd.to_numeric(data['Volume'].replace({',':''},regex=True))
    data['Max']=pd.to_numeric(data['Volume'].replace({',':''},regex=True))
    analyze_timeframes(data, symbol)

import requests
from bs4 import BeautifulSoup
import pandas as pd
from transformers import pipeline
import os
import matplotlib.pyplot as plt

news_dir = "MSE_news"
if not os.path.exists(news_dir):
    os.makedirs(news_dir)

def scrape_news(symbol):
    url = f"https://www.mse.mk/en/symbol/{symbol}"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to retrieve news for {symbol}. Status code: {response.status_code}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")
    news_items = []

    articles = soup.find_all("div", class_="container-seinet")
    for article in articles:
        title = article.find("h2").text.strip() if article.find("h2") else "No Title"
        date = article.find("time").text.strip() if article.find("time") else "No Date"
        link = article.find("a", href=True)["href"] if article.find("a", href=True) else "No Link"
        news_items.append({"Date": date, "Title": title, "Link": link})
    return news_items

# Function to analyze sentiment using transformers pipeline
def analyze_sentiment(news_df):
    classifier = pipeline("sentiment-analysis")
    def get_sentiment(text):
        result = classifier(text)[0]
        polarity = result['score'] if result['label'] == "POSITIVE" else -result['score']
        return polarity

    news_df["Sentiment"] = news_df["Title"].apply(get_sentiment)
    news_df["Sentiment_Category"] = news_df["Sentiment"].apply(
        lambda score: "Positive" if score > 0 else "Negative" if score < 0 else "Neutral"
    )
    return news_df

# Function to generate recommendations based on sentiment
def generate_recommendations(news_df):
    sentiment_counts = news_df["Sentiment_Category"].value_counts()
    positive_count = sentiment_counts.get("Positive", 0)
    negative_count = sentiment_counts.get("Negative", 0)

    if positive_count > negative_count:
        return "Buy"
    elif negative_count > positive_count:
        return "Sell"
    else:
        return "Hold"

for symbol in cs:
    print(f"Scraping news for {symbol}...")
    news = scrape_news(symbol)
    if not news:
        print(f"No news found for {symbol}.")
        continue


    news_df = pd.DataFrame(news)
    news_df.to_csv(f"{news_dir}/{symbol}_news.csv", index=False)


    news_df = analyze_sentiment(news_df)
    print(news_df)


    recommendation = generate_recommendations(news_df)
    print(f"Recommendation for {symbol}: {recommendation}")


    plt.figure(figsize=(10, 6))
    color_mapping = {"Positive": "green", "Negative": "red", "Neutral": "gray"}
    colors = [color_mapping[sentiment] for sentiment in news_df["Sentiment_Category"].value_counts().index]
    news_df["Sentiment_Category"].value_counts().plot(kind="bar", color=colors)
    plt.title(f"Sentiment Analysis for {symbol}")
    plt.xlabel("Sentiment Category")
    plt.ylabel("Number of Articles")
    plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.api.models import Sequential
from keras.api.layers import Input, LSTM, Dense
from sklearn.metrics import r2_score,mean_squared_error
import seaborn as sns
import operator
import numpy as np

for i in cs:
  data=pd.read_csv(f"MSE_data/{i}_data.csv")
  #print(i)
  data["Date"]=pd.to_datetime(data["Date"]).dt.date
  data.set_index(keys=["Date"], inplace=True)

  #bidejki del od firmite nemaat konkretna vredost za last trade price algoritmot ke kazuva ako nema celosno podatoci, ke kazuvadeka nema. Vo sportivno ke presmetuva se shto treba
  if data['Last trade price'].isnull().all():
    print(f"{i} company doesn't have any iformation about last trade price")
  else:

    #print(data.info())
    data['Last trade price']=pd.to_numeric(data['Last trade price'].replace({',':''},regex=True))
    data['Avg. Price']=pd.to_numeric(data['Avg. Price'].replace({',':''},regex=True))
    data['Turnover in BEST in denars']=pd.to_numeric(data['Turnover in BEST in denars'].replace({',':''},regex=True))
    data['Total turnover in denars']=pd.to_numeric(data['Total turnover in denars'].replace({',':''},regex=True))
    data['Volume']=pd.to_numeric(data['Volume'].replace({',':''},regex=True))
    #print(data.info())
    data=data.drop(columns='Max')
    data=data.drop(columns='Min')


    data.index = pd.to_datetime(data.index)
    duplicates = data[data.index.duplicated(keep=False)]


    data.index = pd.to_datetime(data.index, errors="coerce")
    data = data.dropna(subset=["Last trade price"])
    data = data[~data.index.duplicated(keep="first")]
    data = data.sort_index(ascending=True)

    if data.empty:
            print(f"{i} company data is empty after cleaning.")
            continue

    lag=7
    periods=range(lag,0,-1)
    data1=data[['Last trade price']].copy()
    data.shift(periods=periods)
    data=pd.concat([data,data.shift(periods=periods)],axis=1)
    data.dropna(axis=0, inplace=True)
    #data.drop(columns='Last trade price')
    data=data.drop(columns='Avg. Price')
    data=data.drop(columns='%chg.')
    data=data.drop(columns='Volume')
    data=data.drop(columns='Turnover in BEST in denars')
    data=data.drop(columns='Total turnover in denars')

    x=data[:]
    x=x.drop(columns='Last trade price')
    #print(x.shape)
    y=data['Last trade price']

    if len(x) <= 1:  # Check if the DataFrame has at least two samples
            print(f"{i} company doesn't have enough data after preprocessing.")
            continue

    train_x,test_x,train_y,test_y=train_test_split(x,y,train_size=0.70,shuffle=False)

    #print(data.info())

    scaler = MinMaxScaler()
    train_x = scaler.fit_transform(train_x)
    test_x = scaler.transform(test_x)
    #print(train_x.shape)

    pad_size = lag - (train_x.shape[1] % lag)
    if pad_size != lag:  # If padding is needed
      train_x = np.pad(train_x, ((0, 0), (0, pad_size)), mode='constant', constant_values=0)
      test_x = np.pad(test_x, ((0, 0), (0, pad_size)), mode='constant', constant_values=0)

    train_x = train_x.reshape(train_x.shape[0], lag, (train_x.shape[1] // lag))
    test_x = test_x.reshape(test_x.shape[0], lag, (test_x.shape[1] // lag))

    #print((train_x.shape[1], train_x.shape[2]))

    model = Sequential([
      Input((train_x.shape[1], train_x.shape[2],)),
      LSTM(64, activation="relu", return_sequences=True),
      LSTM(32, activation="relu"),
      Dense(1, activation="linear")
    ])
    model.compile(
      loss="mean_squared_error",
      optimizer="adam",
      metrics=["mean_squared_error"],
    )
    if len(train_x)<5:
      print("Dataset too small for training. Please provide more data.")
    else:
      history = model.fit(train_x, train_y, validation_split=0.3, epochs=20, batch_size=7,shuffle=False)
      pred_y = model.predict(test_x)
    #sns.lineplot(history.history['loss'][1:], label='loss')
    #sns.lineplot(history.history['val_loss'][1:], label='val_loss')
      resultR2=r2_score(test_y, pred_y)
      print(resultR2)
      resultMSE=mean_squared_error(test_y,pred_y)
      print(resultMSE)